---
title: "Practical Machine Learning"
author: "Luis Felipe Rogieri"
date: "16/07/2019"
output: html_document
---

# Prediction Assignment Writeup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

The goal of this project is, using data downloaded from the following links:

Training Data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

Test data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv 

And predict the manner in which they did the exercice represented by the "classe" variable in the training set.

## Data preparation

Using the following code we can download and import data.

```{r}
URL1 <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
dest<-getwd()
download.file(URL1,paste0(dest,"/pml-training.csv"))
training<-read.csv("pml-training.csv")

URL2 <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(URL2,paste0(dest,"/pml-testing.csv"))
test<-read.csv("pml-testing.csv")

```

The information below was subtracted from the link http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/harand and will help to understand the problem:

In this work we first define quality of execution and investigate three aspects that pertain to qualitative activity recognition: the problem of specifying correct execution, the automatic and robust detection of execution mistakes, and how to provide feedback on the quality of execution to the user. We tried out an on-body sensing approach (dataset here), but also an "ambient sensing approach" (by using Microsoft Kinect - dataset still unavailable)

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:

- Class A - exactly according to the specification

- Class B - throwing the elbows to the front 

- Class C - lifting the dumbbell only halfway

- Class D - lowering the dumbbell only halfway 

- Class E - throwing the hips to the front.

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

## Cross Validation

Cross-validation will be performed by subsampling our training data set randomly without replacement into 2 subsamples: subTraining data (75%) and subTesting data (25%). Our models will be fitted on the subTraining data set, and tested on the subTesting data. Once the most accurate model is choosen, it will be tested on the original Testing data set.

loading libraries and seed:

```{r}
suppressMessages(library(caret))
suppressMessages(library(randomForest))
suppressMessages(library(rpart))
suppressMessages(library(rpart.plot))
suppressMessages(library(RColorBrewer))
suppressMessages(library(rattle))
set.seed(1205)

```

Spliting data for cross validation:

```{r}
inTrain <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
subTraining <- training[inTrain, ]
subTesting <- training[-inTrain, ]
dim(subTraining) 
dim(subTesting)
```

## Cleaning the data

1. Eliminating NearZeroVariance Variables
2. Eliminating ID variable
3. Eliminating Variables with too many NAs

```{r}
DataNZV <- nearZeroVar(subTraining, saveMetrics=TRUE)
NZVvars <- names(subTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt","kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt","max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm","var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm","stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm","kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm","max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm","kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell","skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell","amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm", "skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm", "max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm", "amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm", "avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm", "stddev_yaw_forearm", "var_yaw_forearm")

subTraining <- subTraining[!NZVvars]

subTraining <- subTraining[c(-1)]

training_clean <- subTraining 
for(i in 1:length(subTraining)) { 
        if( sum( is.na( subTraining[, i] ) ) /nrow(subTraining) >= .6 ) { 
        for(j in 1:length(training_clean)) {
            if( length( grep(names(subTraining[i]), names(training_clean)[j]) ) ==1)  {
                training_clean <- training_clean[ , -j]
            }   
        } 
    }
}
```

And now cleaning tests data sets too:

```{r}

clean1 <- colnames(training_clean)
testing_clean <- subTesting[clean1]
clean2 <- colnames(training_clean[, -58])
test_clean <- test[clean2]

```
## Fiting and coparing Models

### 1.Decision Tree

Fiting a decision tree to the clean training data  

```{r}
modFit1 <- rpart(classe ~ ., data=training_clean, method="class")
```

```{r, echo=FALSE}
fancyRpartPlot(modFit1)
```

...and predicting whith testing data

```{r}
predictions1 <- predict(modFit1, testing_clean, type = "class")
confusionMatrix(predictions1, testing_clean$classe)
```

### 2.Randon Forest

Fiting a Randon Forest to the clean training data

```{r}
modFit2 <-randomForest(classe ~ ., data=training_clean)
```

...and predicting whith testing data

```{r}
predictions2 <- predict(modFit2, testing_clean, type = "class")
confusionMatrix(predictions2, testing_clean$classe)
```

### 3.Comparing Models

As we can see in the confusion matrix above the Randon forrest fitted much better than de decision tree model. The accuracy of the second model was greater than the first one.

## Applying the chosen model to the test data set

Concluding this paper I will use the randon forrest model to predict the out of sample test data set.

```{r}
for (i in 1:length(test_clean) ) {
        for(j in 1:length(training_clean)) {
        if( length( grep(names(training_clean[i]), names(test_clean)[j]) ) ==1)  {
            class(test_clean[j]) <- class(training_clean[i])
        }      
    }      
}


test_clean <- rbind(training_clean[2, -58] , test_clean)
test_clean <- test_clean[-1,]

predictions3 <- predict(modFit2, test_clean, type = "class")
```
